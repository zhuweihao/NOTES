1. java基础

java 运行时 指定 classpath 外部配置文件，将替换内部的配置文件

该功能常用于灵活的配置文件更新

设计个maven 小项目，验证这个功能


2. flink sql 验证 upsert 效果
    a. kafka
    b. mysql
    c. 使用 flink SQL ，将 kafka 数据 抽取到 MySQL
    d. 设置MySQL主键，观察 是否增量更新

设计maven 小项目，本地运行，不用提交集群，验证 效果


3. hive sprk udf 体系
 整理 hive spark udf 函数体系
    包括 如何自定义开发，如何集成

#### Maven打包

[(19条消息) Maven 的打包方式_RonzL的博客-CSDN博客_maven打包](https://blog.csdn.net/zhuxian1277/article/details/119880760)

1. 让项目作为一个依赖提供给他人使用，则将项目打为 “小包”；
2. 项目打包的 Jar 包可以作为一个独立服务运行，则将项目打为 “大包”。

这里记录一种大包的打包方式，其他方法未经实验，详情见上面的链接

```xml
<build>
        <!-- 项目最终打包成的名字 -->
        <finalName>${project.artifactId}</finalName>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <configuration>
                    <archive>
                        <manifest>
                            <!-- 会在 MANIFEST.MF 中生成 Class-Path 项 -->
                            <!-- 系统会根据 Class-Path 项配置的路径加载依赖 -->
                            <addClasspath>true</addClasspath>
                            <!-- 指定依赖包所在目录，相对于项目最终 Jar 包的路径 -->
                            <classpathPrefix>lib/</classpathPrefix>
                            <!-- 指定 MainClass -->
                            <mainClass>com.zhuweihao.ClassPathMission</mainClass>
                        </manifest>
                    </archive>
                </configuration>
            </plugin>

            <!-- 配置依赖包 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <!-- 相当于执行 mvn 命令，将依赖打包到指定目录 -->
                <executions>
                    <execution>
                        <id>copy-dependencies</id>
                        <phase>package</phase>
                        <goals>
                            <goal>copy-dependencies</goal>
                        </goals>
                        <configuration>
                            <!--将依赖打包至 target 下的 lib 目录-->
                            <outputDirectory>${project.build.directory}/lib</outputDirectory>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
```

这种打包方式的目录结构如下

##### ![image-20220616110205028](https://s2.loli.net/2022/06/16/otF8mVCfxn7UPAg.png)

#### java运行时更换配置文件

![image-20220616110625052](https://s2.loli.net/2022/06/16/JipyPejoQwLW5nt.png)

注意：

JVM类加载器在jdk9之后的版本中产生了一些变化，详见

[(19条消息) 最新版JDK17下的JVM类加载器原理详解_JavaEdge.的博客-CSDN博客_jdk17下载](https://blog.csdn.net/qq_33589510/article/details/105250625)

在jdk17中运行下列代码会报错

```java
printPath("java.home");
//printPath("sun.boot.class.path");
//printPath("java.ext.dirs");
printPath("java.class.path");

public static void printPath(String name) {
        System.out.println(name + ":");
        String[] paths = System.getProperty(name).split(File.pathSeparator);
        for(String path : paths) {
            System.out.println("- " + path);
        }
    }
```

```
Exception in thread "main" java.lang.NullPointerException: Cannot invoke "String.split(String)" because the return value of "java.lang.System.getProperty(String)" is null
	at com.zhuweihao.ClassPathMission.printPath(ClassPathMission.java:54)
	at com.zhuweihao.ClassPathMission.main(ClassPathMission.java:18)
```





checkpoint设置

偏移量维护-两种方式

checkpoint同步偏移



env和tableenv执行拓扑不同，

flink sql 和 table转stream 同时使用会出现两套拓扑（只有在提交集群模式才可以看出来）

flink_local_web



本机spark程序



- udf

- udaf
- udtf



hive应用



接口形式  

调用



分布式系统如何支持并行计算



能不能通过全类名反射调用udf
