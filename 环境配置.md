# CentOS虚拟机安装

1. 官网下载VMware，百度密钥即可解锁
2. 官网下载CentOS镜像文件，下载速度较慢，建议挂网或从国内的镜像网站下载
3. VMware参照引导创建虚拟机即可，注意：
   - 虚拟机内存建议最小3G，否则影响使用体验
   - 网络类型选择桥接网络
   - 磁盘大小建议资源允许范围内越大越好，以防后期出现问题
4. 设置虚拟机使用准备好的ios镜像文件，然后开机即可开始系统安装<img src="https://s2.loli.net/2022/06/09/KUO1Ftcd7sZaHoJ.png" alt="image-20220609145216094" style="zoom:80%;" />
5. 对上图中所有内容进行配置，注意分区设置选择自定义分区<img src="https://s2.loli.net/2022/06/09/vUcAN5EtohiyOMQ.png" alt="image-20220609145458323" style="zoom:80%;" />

防火墙设置

```
#查看防火墙状态
systemctl status firewalld
#关闭防火墙
systemctl stop firewalld
#禁用防火墙
systemctl disable firewalld
```

如果此时ping主机不能通过需要对主机防火墙进行设置，如下

![image-20220614140633341](https://s2.loli.net/2022/06/14/RkAMpgFNQeyG95z.png)

禁用SELINUX

```
vi /etc/selinux/config
```

<img src="https://s2.loli.net/2022/06/13/bd7yBZvhmrkQOnE.png" alt="image-20220613161355284" style="zoom:67%;" />

图中为修改后的效果，应该修改的地方为SELINUX=后面的内容，将其改为disable

SSH配置：

检查ssh client和ssh server是否安装

```
rpm -qa | grep ssh
```

![image-20220613161948625](https://s2.loli.net/2022/06/13/xad7IO8jCl4oQcy.png)

如果没有使用如下命令进行安装

```
sudo yum install openssh-clients
sudo yum install openssh-server
```

测试SSH是否可用

```
ssh localhost
```

此时会有如下提示(SSH首次登陆提示)，输入 yes ，然后按提示输入密码 

![image-20220613162403564](https://s2.loli.net/2022/06/13/yEouaGJ6iv3YkWs.png)

首先输入 exit 退出刚才的 ssh，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：

```
exit                           # 退出刚才的 ssh localhost
cd ~/.ssh/                     # 若没有该目录，请先执行一次ssh localhost
ssh-keygen -t rsa              # 会有提示，都按回车就可以
cat id_rsa.pub >> authorized_keys  # 加入授权
chmod 600 ./authorized_keys    # 修改文件权限
```

<img src="https://s2.loli.net/2022/06/13/jyHWx3AKB5E9e4b.png" alt="image-20220613162617165" style="zoom:67%;" />

此时再用 ssh localhost 命令，无需输入密码就可以直接登陆了，如下图所示

![image-20220613162753911](https://s2.loli.net/2022/06/13/bu3O6TIASRwahHf.png)

在 Linux 系统中，~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录，例如我的用户名为zhuweihao，则~就代表/home/zhuweihao

## 虚拟机网络错误

Centos8 不能重启网络

![image-20220919140533235](C:\Users\ZWH\AppData\Roaming\Typora\typora-user-images\image-20220919140533235.png)

原因是由于 centos8 已经替换了原来的network, 新版的叫：NetworkManager
所以用这个命令就可以重启了 systemctl restart NetworkManager

相关命令

```
systemctl restart NetworkManager			重启网络
systemctl status NetworkManager				查看网络状态
service NetworkManager stop 				关闭NetworkManager
chkconfig NetworkManager off 				禁止开机启动
chkconfig NetworkManager on			 		开机启动
service NetworkManager start 				临时启动
chkconfig NetworkManager off 			 	永久关闭托管工具
```

centos8网卡服务由nmcli进行管理。

```
进入网卡配置目录
[root@master network-scripts]# cd /etc/sysconfig/network-scripts/
[root@master network-scripts]# ls
ifcfg-ens160
编辑网卡配置文件
[root@master network-scripts]# vim ifcfg-ens160
重载网卡配置
[root@master network-scripts]# nmcli c reload ens160
重启网卡
[root@master network-scripts]# nmcli c up ens160
```

相关命令

```
查看网卡信息
nmcli connection
显示具体的网络接口信息
nmcli connection show ens160
显示所有活动连接
nmcli connection show --active
```

详情见：[(22条消息) centos8之网卡配置修改及管理_恒悦sunsite的博客-CSDN博客_centos8重启网卡命令](https://blog.csdn.net/carefree2005/article/details/114396600)

nmcli操作介绍：https://zhuanlan.zhihu.com/p/52731316

问题：重启网卡报错device lo not available because device is strictly unmanaged

解决办法：

```
查看托管状态
nmcli n
显示 disabled 则为本文遇到的问题，如果是 enabled 则可以不用往下看了
开启 托管
nmcli n on
```

重启

```
systemctl restart NetworkManager
或
reboot
```



# CentOS 8安装mysql 8

##### 安装

方式一：

```
sudo dnf install @mysql
```

这种方式未经过成功的实践，CentOS 8上提供有MySQL 8，但是该种方法不太稳定

<img src="https://s2.loli.net/2022/06/13/DAP7rW3KBX8Q5zs.png" alt="image-20220609163048836" style="zoom:80%;" />

会产生报错

```
错误：Failed to download metadata for repo 'appstream': Cannot download repomd.xml: Cannot download repodata/repomd.xml: All mirrors were tried
```

方式二：

<img src="https://s2.loli.net/2022/06/09/WQV4IpA6XbhDUNJ.png" alt="image-20220609164321143" style="zoom: 67%;" />

官网下载合适的离线包，解压

依次执行下列命令进行安装

```
sudo rpm -ivh mysql-community-common-8.0.29-1.el8.x86_64.rpm
sudo rpm -ivh mysql-community-client-plugins-8.0.29-1.el8.x86_64.rpm
sudo rpm -ivh mysql-community-libs-8.0.29-1.el8.x86_64.rpm
sudo rpm -ivh mysql-community-client-8.0.29-1.el8.x86_64.rpm
sudo rpm -ivh mysql-community-icu-data-files-8.0.29-1.el8.x86_64.rpm
sudo rpm -ivh mysql-community-server-8.0.29-1.el8.x86_64.rpm
```

##### 配置

安装完成后，启动 MySQL 服务，并设置开机自启

```
sudo systemctl enable --now mysqld
```

检查MySQL状态

```
sudo systemctl status mysqld
```

获取MySQL初始密码

```
cat /var/log/mysqld.log | grep password
```

登录MySQL数据库

```
mysql -u root -p
```

接下来需要修改密码，这里需要注意如果直接使用下面的方法修改密码会报错：

```
ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '123456';
```

```
ERROR 1819 (HY000): Your password does not satisfy the current policy requirements
```

使用如下命令命令可以查看默认的密码规则

```
SHOW VARIABLES LIKE 'validate_password%';
```

![image-20220609170015811](https://s2.loli.net/2022/06/09/4mgVkZTURAwqON9.png)

默认的密码规则如上图所示，故需要先修改一个符合条件密码，然后再利用下列命令对密码规则进行修改

```
set global validate_password.policy=LOW;
set global validate_password.length=5;
```

需要注意MySQL 5与MySQL 8中密码规则的系统变量名不同，若使用MySQL 5则应使用如下命令

```
set global validate_password_policy=LOW;
set global validate_password_length=5;
```

修改后的密码规则如下图所示,

![image-20220609165945514](https://s2.loli.net/2022/06/09/MYaiU5AulBVC7df.png)

现在修改密码后就可以使用新密码进行登录了

# JDK17配置

如果有图形化界面可直接到ORACLE官网下载相应版本JDK后进行后续配置，或者采用如下方法

```
cd ~/opt
wget https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.tar.gz
```

解压压缩包，删除压缩包

```
tar -zxvf jdk-17_linux-x64_bin.tar.gz 
rm -rf jdk-17_linux-x64_bin.tar.gz 
```

配置环境变量：

方法一：

直接编辑profile文件，在文件最下方添加相关设置，注意jdk版本不同需进行相关修改

```
vim /etc/profile
export JAVA_HOME=~/opt/jdk-17.0.3.1
export PATH=$PATH:$JAVA_HOME/bin;
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar;
```

方法二：

在/etc/profile.d 目录中存放的是一些应用程序所需的启动脚本，其中包括了颜色、语言、less、vim及which等命令的一些附加设置。

这些脚本文件之所以能够 被自动执行，是因为在/etc/profile 中使用一个for循环语句来调用这些脚本。而这些脚本文件是用来设置一些变量和运行一些初始化过程的。
作为额外的环境变量，使用方法二可以避免污染初始环境变量

```
cd /etc/profile.d
vim java.sh
export JAVA_HOME=~/opt/jdk-17.0.3.1
export PATH=$PATH:$JAVA_HOME/bin;
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar;
#保存后
chmod 755 /etc/profile.d/java.sh
```

最后重载环境变量

```
source /etc/profile
```



# Hadoop安装与配置

从镜像网站下载相应版本hadoop

[Index of /apache/hadoop/common (cnnic.cn)](https://mirrors.cnnic.cn/apache/hadoop/common/)

下载后进行解压

配置环境变量，方法与JDK配置时相似

```
cd /etc/profile.d
sudo vim hadoop.sh
#将下列代码插入后保存
export HADOOP_HOME=~/opt/hadoop-3.3.3
export PATH=$HADOOP_HOME/bin:$PATH
#重新加载环境变量
source /etc/profile
```

![image-20220614141604846](https://s2.loli.net/2022/06/14/sxv4YluLybZgJa5.png)

验证hadoop是否配置成功

```
hadoop version
```

![image-20220614141825278](https://s2.loli.net/2022/06/14/wlZVLUhMAFi1WGu.png)

伪分布设置

配置环境变量

```
vim ~/.bashrc
#将下列内容添加进去
export HADOOP_HOME=~/opt/hadoop-3.3.3
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

source ~/.bashrc
```



Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。

```
cd ~/opt/hadoop-3.3.3/etc/hadoop
ll
```

![image-20220614143246760](https://s2.loli.net/2022/06/14/tnCqo5TDd7QiZ1g.png)

接下来要配置的文件均使用了下列命令进行权限的管理，就不全部列出了

```
chmod 777 core-site.xml
```



- core-site.xml

```
vim core-site.xml
#添加如下配置
<configuration>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/home/zhuweihao/opt/hadoop-3.3.3/tmp</value>
                <description>Abase for other temporary directories.</description>
        </property>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://localhost:9000</value>
        </property>
</configuration>
```

- hdfs-site.xml

```
vim hdfs-site.xml
#添加如下配置
<configuration>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/home/zhuweihao/opt/hadoop-3.3.3/tmp/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/home/zhuweihao/opt/hadoop-3.3.3/tmp/dfs/data</value>
        </property>
        <property>
                <name>dfs.secondary.http.address</name>
                <!--这里是你自己的ip，端口默认-->
                <value>dfs://localhost:50070</value>
        </property>
</configuration>
```

- mapred-site.xml

```
vim mapred-site.xml
#添加如下配置
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapred.job.tracker.http.address</name>
                <value>0.0.0.0:50030</value>
        </property>

        <property>
                <name>mapred.task.tracker.http.address</name>
                <value>0.0.0.0:50060</value>
        </property>
        <property>
                <name>mapreduce.admin.user.env</name>
                <value>HADOOP_MAPRED_HOME=$HADOOP_COMMON_HOME</value>
        </property>
        <property>
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=$HADOOP_COMMON_HOME</value>
        </property>
</configuration>
```

- yarn-site.xml

```
vim yarn-site.xml
#添加如下配置
<configuration>
        <property>
                 <name>yarn.resourcemanager.hostname</name>
                <!-- 自己的ip端口默认 -->
                <value>hdfs://localhost:9000</value>
        </property>
        <!-- reducer获取数据的方式 -->
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
<!-- Site specific YARN configuration properties -->

</configuration>
```

- hadoop-env.sh

```
vim hadoop-env.sh
#添加如下配置
export JAVA_HOME=~/opt/jdk/jdk-11.0.15.1
```



初始化hadoop文件格式（此图为网图，我自己的当时没截图）

```
hadoop namenode -format
```

![](https://s2.loli.net/2022/06/14/eBQ479iDRF3mWdo.png)

启动所有进程

```
start-all.sh
# 启动yarn
$HADOOP_HOME/sbin/start-yarn.sh
```

使用“ start-dfs.sh ”开启 NaneNode 和 DataNode 守护进程：

```
start-dfs.sh
```

![image-20220614150020371](https://s2.loli.net/2022/06/14/MbRr2vkqSYAGHaV.png)

```
#查看启动的进程
jps
```

![image-20220614150121542](https://s2.loli.net/2022/06/14/PqzkrI92BA8paHF.png)

成功启动后可以通过http://localhost:50070查看相关信息

<img src="https://s2.loli.net/2022/06/14/UD3x1IWt54fRH7G.png" alt="image-20220614155300167" style="zoom: 67%;" />



# kafka安装配置

新版本的kafka已经不需要单独安装zookeeper

[Index of /apache/kafka (cnnic.cn)](https://mirrors.cnnic.cn/apache/kafka/)

下载新版kafka并且解压

对config目录下的server.properties进行配置

```
vim config/server.properties
#配置如下
listeners=PLAINTEXT://localhost:9092
log.dirs=/home/zhuweihao/opt/kafka/logs
```

![image-20220614152428655](https://s2.loli.net/2022/06/14/CJoAKpkNih8PTsa.png)

以守护进程启动zookeeper

```
bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
```

接着启动kafka

```
bin/kafka-server-start.sh -daemon config/server.properties
```

![image-20220614152710606](https://s2.loli.net/2022/06/14/YGQ8eBHyWvERAUo.png)

QuorumPeerMain是zookeeper集群的启动类，用来加载配置启动QuorumPeer线程的



接下来可以对kafka进行测试

创建topic：

```
bin/kafka-topics.sh --create --topic test1 --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
```

查看topic

```
bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

删除topic

```
bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic test1
```



```
bin/kafka-console-producer.sh --topic test1 --bootstrap-server localhost:9092
```

![image-20220614153447614](https://s2.loli.net/2022/06/14/5rPOBTdqGeC8mwv.png)

```
bin/kafka-console-consumer.sh --topic test1 --from-beginning --bootstrap-server localhost:9092
```

![image-20220614153647240](https://s2.loli.net/2022/06/14/xQL6ivXsElPRpn7.png)

![image-20220614153708541](https://s2.loli.net/2022/06/14/K53mSTlbMXB1IwA.png)





# Flink安装部署

前置工作如下：

关闭防火墙，关闭selinux，安装jdk，更改主机名，更改主机名与IP地址的映射关系，ssh免密码登录等

##### Flink Local模式部署

local模式不需要进行任何配置，只要保证jdk的正确安装就可以

```
#启动flink
start-cluster.sh
#启动后多了如下两个进程
17992 StandaloneSessionClusterEntrypoint
18264 TaskManagerRunner
```

![image-20220614154824774](https://s2.loli.net/2022/06/14/sQvqJuYl5mfbNEG.png)

可通过http://localhost:8081/#/overview查看相关信息

<img src="https://s2.loli.net/2022/06/14/XpnDVIKcqEJQLSv.png" alt="image-20220614155220145" style="zoom: 50%;" />

测试

```
bin/flink run examples/batch/WordCount.jar
bin/flink run /home/zhuweihao/opt/data/Flink-1.0-SNAPSHOT.jar
```

<img src="https://s2.loli.net/2022/06/14/JW7UyrbdMsGX2RB.png" alt="image-20220614155958203" style="zoom:67%;" />

关闭flink

```
stop-cluster.sh
```

![image-20220614160232051](https://s2.loli.net/2022/06/14/h3L1AeIauwiXHt8.png)



# Docker

https://www.runoob.com/docker/docker-tutorial.html



https://docs.docker.com/engine/install/centos/



```shell
yum install -y gcc
yum install -y gcc-c++
yum install -y yum-utils
```



```
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```



```
yum makecache fast
```



```
yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin
```



```
systemctl start docker
systemctl enable docker
docker run hello-world
```



```
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://w1aza1f8.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```



```
yum remove docker-ce docker-ce-cli containerd.io docker-compose-plugin
rm -rf /var/lib/docker
rm -rf /var/lib/containerd
```



## MySQL

```
docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=03283x -d mysql:8.0
docker ps
docker exec -it msyql bash
mysql -uroot -p
```

![image-20221126151714984](环境配置.assets/image-20221126151714984.png)



```
docker run --name zwh_mysql -p 3309:3306 --privileged=true -v /home/zwh/mysql/log:/var/log/mysql -v /home/zwh/mysql/data:/var/lib/mysql -v /home/zwh/mysql/conf:/etc/mysql -e MYSQL_ROOT_PASSWORD=03283x -d mysql:8.0.27
```

创建两个MySQL容器，其中一个容器不要挂载配置文件的目录，将该容器相应配置文件cp到宿主机目录下

```
docker cp 容器id:/etc/mysql/ conf
```

然后删除没有挂载配置文件的容器，在宿主机进行相关配置后，重启另一个容器



> 实战中，运行mysql容器要挂载数据卷，将数据在本机进行备份，防止容器误删丢失数据
>
> ![image-20221126152823089](环境配置.assets/image-20221126152823089.png)
>
> 上面为尚硅谷5.7版本教程
>
> https://dev.mysql.com/doc/refman/8.0/en/docker-mysql-more-topics.html
>
> 具体可查看官网，docker会自动进行数据卷的挂载，可通过如下命令进行查看
>
> ```
> docker inspect mysql
> ```
>
> ![image-20221126163634202](环境配置.assets/image-20221126163634202.png)
>
> 或者通过如下命令进行自定义
>
> ```
> #官网示例
> docker run --name=mysql1 \
> --mount type=bind,src=/path-on-host-machine/my.cnf,dst=/etc/my.cnf \
> --mount type=bind,src=/path-on-host-machine/datadir,dst=/var/lib/mysql \
> -d mysql/mysql-server:tag
> ```
>
> 



![image-20221129100129825](环境配置.assets/image-20221129100129825.png)

修改配置文件

```
vim my.cnf
[mysqld]
local_infile=ON

vim conf.d/mysql
[mysql]
local_infile=ON

SHOW GLOBAL VARIABLES LIKE 'local_infile';
```





## Doris



```
docker run -itd --name doris apache/doris:build-env-ldb-toolchain-latest
```



```
docker run -itd --name doris -v /root/.m2:/root/.m2 -v /opt/apache-doris-1.1.4-src:/root/apache-doris-1.1.4-src apache/doris:1.1.4
```

配置挂载目录可以避免容器删除后下一次重新编译时再次下载相关依赖



# 集群配置

## Hadoop

|      | hadoop01               | hadoop02                         | hadoop03                        |
| ---- | ---------------------- | -------------------------------- | ------------------------------- |
| HDFS | NameNode<br />DataNode | DataNode                         | SecondaryNameNode<br />DataNode |
| YARN | NodeManager            | ResourceManager<br />NodeManager | NodeManager                     |

xsync集群分发脚本

```shell
#!/bin/bash

#1. 判断参数个数
if [ $# -lt 1 ]
then
    echo Not Enough Arguement!
    exit;
fi

#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
    echo ====================  $host  ====================
    #3. 遍历所有目录，挨个发送

    for file in $@
    do
        #4. 判断文件是否存在
        if [ -e $file ]
            then
                #5. 获取父目录
                pdir=$(cd -P $(dirname $file); pwd)

                #6. 获取当前文件的名称
                fname=$(basename $file)
                ssh $host "mkdir -p $pdir"
                rsync -av $pdir/$fname $host:$pdir
            else
                echo $file does not exists!
        fi
    done
done
```

core-site.xml

```xml
<configuration>
    <!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:8020</value>
    </property>

    <!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop-3.2.4/data</value>
    </property>

    <!-- 配置HDFS网页登录使用的静态用户为root -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>
</configuration>
```

hdfs-site.xml

```xml
<configuration>
    <!-- nn web端访问地址-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:9870</value>
    </property>
    <!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop03:9868</value>
    </property>
</configuration>
```

yarn-site.xml

```xml
<configuration>

<!-- Site specific YARN configuration properties -->
    <!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop02</value>
    </property>
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    
    <!-- 开启日志聚集功能 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 设置日志聚集服务器地址 -->
    <property>  
        <name>yarn.log.server.url</name>  
        <value>http://hadoop01:19888/jobhistory/logs</value>
    </property>
    <!-- 设置日志保留时间为7天 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>

</configuration>
```

mapred-site.xml

```xml
<configuration>
    <!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop01:10020</value>
    </property>

    <!-- 历史服务器web端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop01:19888</value>
    </property>

</configuration>
```

workers

```shell
hadoop01
hadoop02
hadoop03
```



> ![image-20221125133800629](环境配置.assets/image-20221125133800629.png)
>
> 解决办法：
>
> ```shell
> vim /etc/profile.d/my_env.sh
> ```
>
> ```sh
> export HDFS_NAMENODE_USER=root
> export HDFS_DATANODE_USER=root
> export HDFS_SECONDARYNAMENODE_USER=root
> export YARN_RESOURCEMANAGER_USER=root
> export YARN_NODEMANAGER_USER=root
> ```





| 端口名称                  | Hadoop2.x   | Hadoop3.x         |
| ------------------------- | ----------- | ----------------- |
| NameNode内部通信端口      | 8020 / 9000 | 8020 /  9000/9820 |
| NameNode HTTP UI          | 50070       | 9870              |
| MapReduce查看执行任务端口 | 8088        | 8088              |
| 历史服务器通信端口        | 19888       | 19888             |



hadoop集群启动脚本

```shell
#!/bin/bash

if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi

case $1 in
"start")
        echo " =================== 启动 hadoop集群 ==================="

        echo " --------------- 启动 hdfs ---------------"
        ssh hadoop01 "/opt/hadoop-3.2.4/sbin/start-dfs.sh"
        echo " --------------- 启动 yarn ---------------"
        ssh hadoop02 "/opt/hadoop-3.2.4/sbin/start-yarn.sh"
        echo " --------------- 启动 historyserver ---------------"
        ssh hadoop01 "/opt/hadoop-3.2.4/bin/mapred --daemon start historyserver"
;;
"stop")
        echo " =================== 关闭 hadoop集群 ==================="

        echo " --------------- 关闭 historyserver ---------------"
        ssh hadoop01 "/opt/hadoop-3.2.4/bin/mapred --daemon stop historyserver"
        echo " --------------- 关闭 yarn ---------------"
        ssh hadoop02 "/opt/hadoop-3.2.4/sbin/stop-yarn.sh"
        echo " --------------- 关闭 hdfs ---------------"
        ssh hadoop01 "/opt/hadoop-3.2.4/sbin/stop-dfs.sh"
;;
*)
    echo "Input Args Error..."
;;
esac
```



分发脚本

```shell
#!/bin/bash

#1. 判断参数个数
if [ $# -lt 1 ]
then
    echo Not Enough Arguement!
    exit;
fi

#2. 遍历集群所有机器
for host in hadoop01 hadoop02 hadoop03
do
    echo ====================  $host  ====================
    #3. 遍历所有目录，挨个发送

    for file in $@
    do
        #4. 判断文件是否存在
        if [ -e $file ]
            then
                #5. 获取父目录
                pdir=$(cd -P $(dirname $file); pwd)

                #6. 获取当前文件的名称
                fname=$(basename $file)
                ssh $host "mkdir -p $pdir"
                rsync -av $pdir/$fname $host:$pdir
            else
                echo $file does not exists!
        fi
    done
done

```



## Flink

| hadoop01   | hadoop02    | hadoop03    |
| ---------- | ----------- | ----------- |
| JobManager | TaskManager | TaskManager |



启动脚本

```shell
#!/bin/bash

if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi

case $1 in
"start")
        echo " --------------- 启动 flink ---------------"
        ssh hadoop01 "/opt/flink-1.14.6/bin/start-cluster.sh"
;;
"stop")
        echo " --------------- 停止 flink ---------------"
        ssh hadoop01 "/opt/flink-1.14.6/bin/stop-cluster.sh"
;;
*)
    echo "Input Args Error..."
;;
esac
```



### StandLone

修改flink-conf.xml

```
vim flink-conf.yaml
jobmanager.rpc.address: hadoop01
```

```
vim workers 
hadoop103
hadoop104
```

完成后进行分发

### Yarn

https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/deployment/resource-providers/yarn/

```
sudo vim /etc/profile.d/my_env.sh
export HADOOP_HOME=/opt/hadoop-3.2.4
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_CLASSPATH=`hadoop classpath`
```





## Kafka



| hadoop01 | hadoop02 | hadoop03 |
| -------- | -------- | -------- |
| zk       | zk       | zk       |
| kafka    | kafka    | kafka    |

```shell
vim server.properties

#修改以下内容
log.dirs=/opt/kafka_2.12-3.3.1/data
zookeeper.connect=hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka
```

```shell
vim /etc/profile.d/my_env.sh

#KAFKA_HOME
export KAFKA_HOME=/opt/kafka_2.12-3.3.1
export PATH=$PATH:$KAFKA_HOME/bin
```

配置完成后进行分发，并source环境变量

kafka启动脚本，kafka启动前需要先启动zookeeper

```shell
#! /bin/bash
case $1 in
"start"){
	for i in hadoop01 hadoop02 hadoop03
	do
		echo " --------------- 启动 zookeeper --------------"
		ssh $i "/opt/kafka_2.12-3.3.1/bin/zookeeper-server-start.sh -daemon /opt/kafka_2.12-3.3.1/config/zookeeper.properties"
		echo " --------启动 $i Kafka-------"
		ssh $i "/opt/kafka_2.12-3.3.1/bin/kafka-server-start.sh -daemon /opt/kafka_2.12-3.3.1/config/server.properties"
	done
};;
"stop"){
	for i in hadoop01 hadoop02 hadoop03
	do
		echo " --------停止 $i Kafka-------"
		ssh $i "/opt/kafka_2.12-3.3.1/bin/kafka-server-stop.sh "
		echo " --------------- 停止 zookeeper --------------"
		ssh $i "/opt/kafka_2.12-3.3.1/bin/zookeeper-server-stop.sh"
	done
};;
esac
```





















